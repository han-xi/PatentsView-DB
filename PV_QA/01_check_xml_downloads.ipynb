{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:33:17.057852Z",
     "start_time": "2019-12-10T16:33:17.049838Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from helpers import general_helpers\n",
    "import pytest \n",
    "import pytz\n",
    "\n",
    "project_home = os.environ['PACKAGE_HOME']\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(project_home + '/Development/config.ini')\n",
    "start_date = config['DATES']['START_DATE']\n",
    "end_date = config['DATES']['END_DATE']\n",
    "\n",
    "\n",
    "# set folders\n",
    "rawxml_folder = '{}/raw_data/'.format(config['FOLDERS']['WORKING_FOLDER'])\n",
    "cleanxml_folder = '{}/clean_data/'.format(config['FOLDERS']['WORKING_FOLDER'])\n",
    "parsedxml_folder = '{}/parsed_data/'.format(config['FOLDERS']['WORKING_FOLDER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:32:17.255788Z",
     "start_time": "2019-12-10T16:32:17.248699Z"
    }
   },
   "outputs": [],
   "source": [
    "qa_db = 'patent_QA'\n",
    "new_db = config['DATABASE']['NEW_DB']\n",
    "db_con = general_helpers.connect_to_db(config['DATABASE']['HOST'], config['DATABASE']['USERNAME'], config['DATABASE']['PASSWORD'], qa_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:13:34.413130Z",
     "start_time": "2019-12-10T17:13:34.376240Z"
    }
   },
   "outputs": [],
   "source": [
    "# REQUIRES: start and end timestamps (database format \"YYYYMMDD\")\n",
    "# MODIFIES: nothing \n",
    "# EFFECTS: returns number of weeks data update spans\n",
    "def get_num_weeks(start_timestamp, end_timestamp):\n",
    "    \n",
    "    time_period = end_timestamp - start_timestamp\n",
    "    num_weeks = math.ceil(time_period.days // 7)\n",
    "    \n",
    "    return num_weeks\n",
    "\n",
    "# REQUIRES: data update start & end dates (from config file, database format \"YYYYMMDD\")\n",
    "# MODIFIES: nothing \n",
    "# EFFECTS: returns number of weeks data update spans, all tuesdays of data\n",
    "def get_tuesdays(start_date, end_date):\n",
    "    \n",
    "    # convert dates to datetime objects\n",
    "    start_timestamp = datetime.strptime(start_date,'%Y%m%d')\n",
    "    end_timestamp = datetime.strptime(end_date,'%Y%m%d')\n",
    "\n",
    "    # see how many weeks data spans ( get # of files that should be downloaded)\n",
    "    num_weeks = get_num_weeks(start_timestamp, end_timestamp)\n",
    "    \n",
    "    # now get dates of all tuesdays part of data update\n",
    "    all_tuesdays = []\n",
    "\n",
    "    # need to -1 from start timestamp since it will always be a wednesday\n",
    "    ts = start_timestamp - timedelta(days=1)\n",
    "\n",
    "    for i in range(0, num_weeks):\n",
    "        # get date of next tuesday\n",
    "        ts = ts + timedelta(days=7)\n",
    "        ts_str = datetime.strftime(ts, '%Y%m%d')\n",
    "        all_tuesdays.append(ts_str)\n",
    "        \n",
    "    return num_weeks, all_tuesdays\n",
    "\n",
    "\n",
    "# REQUIRES: db connection, qa_db name, new_db name, data folder path, num_weeks, filetype('raw, clean, or parsed xml')\n",
    "# MODIFIES: nothing\n",
    "# EFFECTS: asserts if # of folders matches expected # for data update\n",
    "def test_num_xmlfiles(db_con, qa_db, new_db, db_table, folder_path, num_weeks, file_type):\n",
    "    \n",
    "    test_type = 'num_files'\n",
    "    \n",
    "    num_xml_files = len(os.listdir(folder_path))\n",
    "\n",
    "    # static - table columns are fixed\n",
    "    stmt_pt1 = \"insert into {0}.{1} (`db_timestamp`,`test_type`, `file_type`, `expected_value`, `actual_value`)\".format(qa_db,db_table)\n",
    "\n",
    "    # dynamic - row to insert changes\n",
    "    stmt_pt2 = \"values ('{0}', '{1}', '{2}', {3}, {4});\".format(new_db, test_type, file_type, num_weeks, num_xml_files)\n",
    "\n",
    "    # insert row into qa table\n",
    "    insert_stmt = stmt_pt1 + stmt_pt2\n",
    "    db_con.execute(insert_stmt)\n",
    "      \n",
    "    assert(num_xml_files == num_weeks)\n",
    "    return\n",
    "\n",
    "# REQUIRES: db connection, qa_db name, new_db name, data folder path, filetype('raw, clean, or parsed xml')\n",
    "# MODIFIES: nothing\n",
    "# EFFECTS: returns \n",
    "def test_size_xml(db_con, qa_db, new_db, db_table, folder_path, file_type):\n",
    "    \n",
    "    test_type = 'size_files'\n",
    "    xml_files = os.listdir(folder_path)\n",
    "\n",
    "    # check each file's size\n",
    "    for fle in xml_files:\n",
    "        \n",
    "        \n",
    "        fle_size = os.stat(folder_path + fle).st_size\n",
    "        \n",
    "        fle_size = round((fle_size/math.pow(10,3)),2)\n",
    "        \n",
    "        # static - table columns are fixed\n",
    "        stmt_pt1 = \"insert into {0}.{1} (`db_timestamp`,`test_type`, `file_type`, `file_name`, `actual_value`)\".format(qa_db,db_table)\n",
    "\n",
    "        # dynamic - row to insert changes\n",
    "        stmt_pt2 = \"values ('{0}', '{1}', '{2}', '{3}', {4});\".format(new_db, test_type, file_type, fle, fle_size)\n",
    "\n",
    "        # insert row into qa table\n",
    "        insert_stmt = stmt_pt1 + stmt_pt2\n",
    "        db_con.execute(insert_stmt)\n",
    "        \n",
    "        \n",
    "        assert(fle_size > 0)\n",
    "        \n",
    "    \n",
    "    return\n",
    "    \n",
    "# REQUIRES: db connection, qa_db name, new_db name, data folder path, tuesdays\n",
    "# MODIFIES: nothing\n",
    "# EFFECTS: returns \n",
    "def test_num_parsedxml(db_con, qa_db, new_db, db_table, folder_path, tuesdays, tables):\n",
    "    test_type = 'num_files'\n",
    "    file_type = 'parsed_xml'\n",
    "    \n",
    "    for tues in tuesdays:\n",
    "        num_parsed_tues_files = len(os.listdir(folder_path + tues + '/'))\n",
    "\n",
    "        # static - table columns are fixed\n",
    "        stmt_pt1 = \"insert into {0}.{1} (`db_timestamp`,`test_type`, `file_type`, `file_name`,  `expected_value`, `actual_value`)\".format(qa_db,db_table)\n",
    "\n",
    "        # dynamic - row to insert changes\n",
    "        stmt_pt2 = \"values ('{0}', '{1}', '{2}', '{3}', {4}, {5});\".format(new_db, test_type, file_type, tues, len(tables), num_parsed_tues_files)\n",
    "\n",
    "        # insert row into qa table\n",
    "        insert_stmt = stmt_pt1 + stmt_pt2\n",
    "\n",
    "        db_con.execute(insert_stmt)\n",
    "\n",
    "        assert num_parsed_tues_files == len(tables)\n",
    "        \n",
    "    return\n",
    "\n",
    "\n",
    "# REQUIRES: db connection, qa_db name, new_db name, data folder path, tuesdays\n",
    "# MODIFIES: nothing\n",
    "# EFFECTS: returns \n",
    "def test_size_parsedxml(db_con, qa_db, new_db, db_table, folder_path, tuesdays, tables):\n",
    "    test_type = 'size_files'\n",
    "    file_type = 'parsed_xml'\n",
    "    # TODO: assess error_counts.csv and error_data.csv\n",
    "    tables = ['application.csv', 'botanic.csv','brf_sum_text.csv','claim.csv','detail_desc_length.csv','detail_desc_text.csv',\n",
    " 'draw_desc_text.csv','figures.csv','foreign_priority.csv','foreigncitation.csv','government_interest.csv','ipcr.csv','mainclass.csv','non_inventor_applicant.csv','otherreference.csv','patent.csv','pct_data.csv','rawassignee.csv','rawexaminer.csv','rawinventor.csv','rawlawyer.csv','rawlocation.csv','rel_app_text.csv','subclass.csv','us_term_of_grant.csv','usapplicationcitation.csv','uspatentcitation.csv', 'uspc.csv','usreldoc.csv']\n",
    "\n",
    "    \n",
    "    for tues in tuesdays:\n",
    "        fp = folder_path + tues + '/'\n",
    "        parsed_tues_files = os.listdir(fp)\n",
    "        \n",
    "        for tbl in parsed_tues_files:\n",
    "            if tbl in tables:\n",
    "                full_tbl_name = tues + '/' + tbl\n",
    "\n",
    "                tbl_size = os.stat(fp + tbl).st_size\n",
    "                print(tbl)\n",
    "                print(tbl_size)\n",
    "                tbl_size = round((tbl_size/math.pow(10,9)),2)\n",
    "\n",
    "                # static - table columns are fixed\n",
    "                stmt_pt1 = \"insert into {0}.{1} (`db_timestamp`,`test_type`, `file_type`, `file_name`, `actual_value`)\".format(qa_db,db_table)\n",
    "\n",
    "                # dynamic - row to insert changes\n",
    "                stmt_pt2 = \"values ('{0}', '{1}', '{2}', '{3}', {4});\".format(new_db, test_type, file_type, full_tbl_name, tbl_size)\n",
    "\n",
    "                # insert row into qa table\n",
    "                insert_stmt = stmt_pt1 + stmt_pt2\n",
    "                db_con.execute(insert_stmt)\n",
    "\n",
    "        \n",
    "                assert(tbl_size > 0)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:13:41.210380Z",
     "start_time": "2019-12-10T17:13:41.190840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weeks of data update:  6\n",
      "Tuesdays are:  ['20190827', '20190903', '20190910', '20190917', '20190924', '20191001']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/dbupdate_data//parsed_data/180717/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-1057f9c226b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#7. Check that each parsed .csv is in each parsed folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mtest_num_parsedxml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_con\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqa_db\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_db\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'01_xml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparsedxml_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabbrev_tues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#8. Check that each parsed .csv in each parsed folder has a size > 0 MB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-136-2aa7db45459b>\u001b[0m in \u001b[0;36mtest_num_parsedxml\u001b[0;34m(db_con, qa_db, new_db, db_table, folder_path, tuesdays, tables)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtues\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtuesdays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mnum_parsed_tues_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtues\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# static - table columns are fixed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/dbupdate_data//parsed_data/180717/'"
     ]
    }
   ],
   "source": [
    "#1. Get span of data update\n",
    "num_weeks, all_tuesdays = get_tuesdays(start_date, end_date)\n",
    "\n",
    "print(\"Number of weeks of data update: \", num_weeks)\n",
    "print(\"Tuesdays are: \", all_tuesdays)\n",
    "\n",
    "all_tuesdays = ['20180717', '20181023', '20181030', '20181106', '20181113', '20181120', '20181127']\n",
    "\n",
    "# #2. Check raw XML files were downloaded\n",
    "# test_num_xmlfiles(db_con, qa_db, new_db, '01_xml', rawxml_folder, num_weeks + 1, 'raw_xml')\n",
    "\n",
    "# #3. Check that raw XML files have size > 0MB\n",
    "# test_size_xml(db_con, qa_db, new_db, '01_xml', rawxml_folder, 'raw_xml')\n",
    "\n",
    "# #4. Check that clean XML files were created\n",
    "# test_num_xmlfiles(db_con, qa_db, new_db, '01_xml', cleanxml__folder, num_weeks + 1, 'clean_xml')\n",
    "\n",
    "# #5. Check that clean XML files have size > 0MB\n",
    "# test_size_xml(db_con, qa_db, new_db, '01_xml', rawxml_folder, 'clean_xml')\n",
    "\n",
    "#6. Check that a parsed XML folder was created for each Tuesday\n",
    "#test_num_xmlfiles(db_con, qa_db, new_db, '01_xml', parsedxml__folder, num_weeks + 1, 'parsed_xml')\n",
    "# convert format from '20180717' to '180717'\n",
    "all_tuesdays = ['20190723'] # for testing\n",
    "abbrev_tuesdays = [x[2:] for x in all_tuesdays]\n",
    "\n",
    "#7. Check that each parsed .csv is in each parsed folder \n",
    "test_num_parsedxml(db_con, qa_db, new_db, '01_xml', parsedxml_folder, abbrev_tues, tables)\n",
    "\n",
    "#8. Check that each parsed .csv in each parsed folder has a size > 0 MB\n",
    "test_size_parsedxml(db_con, qa_db, new_db, '01_xml', parsedxml_folder, abbrev_tuesdays, tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T20:30:12.489225Z",
     "start_time": "2019-12-05T20:30:12.484377Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "db_con.execute(\"insert into {0}.test_num_xmlfiles (`db_timestamp`, `expected_value`, `actual_value`) values ('{1}', 5, 4);\".format(qa_db, new_db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:58:38.180309Z",
     "start_time": "2019-12-10T16:58:38.176025Z"
    }
   },
   "outputs": [],
   "source": [
    "test = all_tuesdays[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T20:30:24.478753Z",
     "start_time": "2019-12-05T20:30:24.471075Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "downloaded_xml_files = os.listdir(rawdata_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:59:18.855435Z",
     "start_time": "2019-12-10T16:59:18.847894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['180717', '181023', '181030', '181106', '181113', '181120', '181127']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "abbrev_tues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:12:59.908337Z",
     "start_time": "2019-12-10T17:12:59.902710Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "447.786"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = 447786\n",
    "z/math.pow(10,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:13:18.807164Z",
     "start_time": "2019-12-10T17:13:18.798216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "447.786"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                                                                                                                                   \n",
    "round(z/math.pow(10,3),3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
